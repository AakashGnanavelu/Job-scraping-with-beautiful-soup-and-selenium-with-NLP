{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c970d9",
   "metadata": {},
   "source": [
    "# Job Scraping in Linkedin with BeautifulSoup and Selenium with Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091ce46",
   "metadata": {},
   "source": [
    "Recently, my friend asked my to help him automate his job serching experience using Python. He wanted my to help him rank the jobs that have a high chance of him being called in for a interview. This mean I had to find all job description of all the jobs, and I have to find the keywords of these job descriptions and check if they are in the resume. Let's Start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df34297",
   "metadata": {},
   "source": [
    "### Part 1 : Getting the information from Linkedin\n",
    "\n",
    "Let us start by calling all the nessary libraries, for this project we are going to mainly be using `selenium` and `BeautifulSoup`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep\n",
    "from time import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b343e",
   "metadata": {},
   "source": [
    "Let us redefine the directory to where you are going to save all your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "os.chdir(r\"C:\\Users\\Aakash\\Desktop\\AAKASH\\Coding Stuff\\Python\\Project\\Linkedin Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d11ad1",
   "metadata": {},
   "source": [
    "Let us define url to serch from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7026f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.linkedin.com/jobs/search/?keywords=senior%20electrical%20engineer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6164237",
   "metadata": {},
   "source": [
    "Install the the webdriver for your prefered website and make sure you save it where your code is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1442ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "sleep(3)\n",
    "action = ActionChains(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2343db",
   "metadata": {},
   "source": [
    "Linkedin has a lazy-loading system. This means that the content is not loaded until we need it. In the case of Linkedin we need to get to scroll down to load more jobs. We will use a for loop to scroll down multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dddffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scroll in range(0, 20):\n",
    "    sleep(2) # Has time to load\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459560bd",
   "metadata": {},
   "source": [
    "Next, using `BeautifulSoup`, let us get the html source code of the page to find the links to the job pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8df4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = driver.page_source\n",
    "soup = BeautifulSoup(source, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070feea",
   "metadata": {},
   "source": [
    "We are going to find the links of the page sources using `.find_all`, using the class names of the `a` tag to help find only the correct link. We append the links into a list so we call it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_link = []\n",
    "\n",
    "for a in soup.find_all('a', 'base-card__full-link',href=True):\n",
    "    job_link.append(a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1a876",
   "metadata": {},
   "source": [
    "Let us go to the pages we extracted before and get all the information we need. In order to find the information we need, we are going to find the tag using their class name. We use `sleep(2)` so that the code waits for few second in order to make sure we do not get classified as a bot in the severs. We run this code we only get the tag, so later we need to extract the text from it. We can see what tag we extract from the fist parameter of the `soup.find()`, and in the case of `raw_job_title` it is `h1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(job_link)):\n",
    "    driver.get(job_link[x])\n",
    "    sleep(2)\n",
    "      \n",
    "    job_source = driver.page_source\n",
    "    soup = BeautifulSoup(job_source, 'lxml')\n",
    "    \n",
    "    raw_job_title.append(soup.find('h1', class_='top-card-layout__title topcard__title'))\n",
    "    raw_company_name.append(soup.find('a', class_ = 'topcard__org-name-link topcard__flavor--black-link'))\n",
    "    raw_location.append(soup.find('span', class_='topcard__flavor topcard__flavor--bullet'))\n",
    "    raw_job_description.append(soup.find('div', class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5'))\n",
    "    raw_level.append(soup.find('span', class_= \"description__job-criteria-text description__job-criteria-text--criteria\")) \n",
    "    raw_function.append(soup.find('span',class_= 'description__job-criteria-text description__job-criteria-text--criteria'))\n",
    "    \n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c386a",
   "metadata": {},
   "source": [
    "Once we are done using the driver, let us close the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37009ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64430ee6",
   "metadata": {},
   "source": [
    "Make sure the informmation got stored correctly. The length of all the lists should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_link)\n",
    "len(raw_job_title)\n",
    "len(raw_company_name)\n",
    "len(raw_location)\n",
    "len(raw_job_description)\n",
    "len(raw_level)\n",
    "len(raw_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8495d6",
   "metadata": {},
   "source": [
    "Let us remove the tags, which leaves behind the text only, which allows us to carry on our Natural language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(0, len(job_link)):\n",
    "    job_title.append(raw_job_title[y].text)\n",
    "    company_name.append(raw_company_name[y].text)\n",
    "    location.append(raw_location[y].text)\n",
    "    level.append(raw_level[y].text)\n",
    "    function.append(raw_function[y].text)\n",
    "    job_description.append(raw_job_description[y].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b000b21",
   "metadata": {},
   "source": [
    "Once again let us make sure the information was stored correctly. The length of all the lists should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7229d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_title)\n",
    "len(company_name)\n",
    "len(location)\n",
    "len(job_description)\n",
    "len(level)\n",
    "len(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6e842",
   "metadata": {},
   "source": [
    "### Part 2: Finding keywords and ranking jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71e468",
   "metadata": {},
   "source": [
    "Let us start by testing some commonly used keyword detection model. We will test the keyword detection system on our resume. We are going to test four different keyword detection systems, which are `Rake`, `gensim`, `yake` and `spacy` models. I did considered using other models such as a deep learning model such as `LSTM`, but I didn't have enough training and testing data in order to build one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63371130",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_file = open('resume.txt', 'r', encoding='utf-8')\n",
    "resume = resume_file.read()\n",
    "resume_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "rake_nltk_var = Rake()\n",
    "rake_nltk_var.extract_keywords_from_text(resume)\n",
    "keyword_extracted = rake_nltk_var.get_ranked_phrases()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "keyword = keywords(resume)\n",
    "result = keyword.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp_md = en_core_web_md.load()\n",
    "nlp_lg = en_core_web_lg.load()\n",
    "\n",
    "doc_md = nlp_md(resume)\n",
    "result_md = list(doc_md.ents)\n",
    "\n",
    "doc_lg = nlp_lg(resume)\n",
    "resume_lg = list(doc_lg.ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "language = 'en'\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 30\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c59f56",
   "metadata": {},
   "source": [
    "Out of the four model tested, I think the Rake-nltk model and the yake models are the best. But we will use the spacy library to find the proper nouns later on, so word like company names don't come into our keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import yake\n",
    "import spacy\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33f32e",
   "metadata": {},
   "source": [
    "Let us continue by removing numbers and special charaters from our job description and the resume, so our keyword model is not case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(job_description)):\n",
    "    job_description[x] = re.sub(\"[^A-Za-z\" \"]+\",\" \",job_description[x]).lower()\n",
    "    job_description[x] = re.sub(\"[0-9\" \"]+\",\" \", job_description[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = re.sub(\"[^A-Za-z\" \"]+\",\" \",resume).lower()\n",
    "resume = re.sub(\"[0-9\" \"]+\",\" \", resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06006c",
   "metadata": {},
   "source": [
    "Let us get a text file with all the words in the english language, so our model doesn't pick up a word that doesn't exsit in the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c77dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_file = open('dict.txt', 'r', encoding='utf-8')\n",
    "dict = dict_file.read()\n",
    "dict_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb5bae",
   "metadata": {},
   "source": [
    "We also have to remove all capitization, special charaters and numbers for the dict file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18267181",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict = re.sub(\"[^A-Za-z\" \"]+\",\" \",dict).lower()\n",
    "raw_dict = re.sub(\"[0-9\" \"]+\",\" \", dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e7059",
   "metadata": {},
   "source": [
    "We are also going to remove stopword from the dictionary, so words like 'I', 'you' and 'about' will be removed. To do this we are going to tokenize the word, and this will spilt all the words, so we can check if it is in the stopword list. For the list itself we are going to use the stopword list provided by the library `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "dict_tokens = word_tokenize(raw_dict)\n",
    "\n",
    "dict = []\n",
    "for w in dict_tokens:\n",
    "    if w not in stop_words:\n",
    "        dict.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58b168",
   "metadata": {},
   "source": [
    "Now that we have prepared all the nessasary items, we can find the keywords of the job descriptions we extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.5\n",
    "numOfKeywords = 30\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "raw_yake_job_keyword = []\n",
    "rake_job_keyword = []\n",
    "\n",
    "for z in range(0, len(job_link)):\n",
    "    raw_yake_job_keyword.append(list(custom_kw_extractor.extract_keywords(job_description[z])))\n",
    "    rake_nltk_var.extract_keywords_from_text(job_description[z])\n",
    "    rake_job_keyword.append(rake_nltk_var.get_ranked_phrases())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a32f44",
   "metadata": {},
   "source": [
    "Now, we have found the keywords in the job description. Next, we need to find the the synonyms of the keywords, as our resume might have the keyword but worded differently. Without the synonym finder, jobs will be graded badly.\n",
    "\n",
    "At the code  below, we first clean the keyword phrase becasue the models we can detemine of a phrase can be multiple words. To clean it we have to check if it is a proper noun, so things like names and places, in it is a actual word, and if it in the stopword. Then, our code find the synonyms for the word and tries to spilt them if the synonyms have multiple words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_num in range(0, len(rake_job_keyword)):\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for word_num in range(0, len(rake_job_keyword[list_num])):\n",
    "        keyword_phrase = rake_job_keyword[list_num][word_num]\n",
    "        word_tokens = word_tokenize(keyword_phrase)\n",
    "        \n",
    "        filter_sentence = []\n",
    "        for w in word_tokens:\n",
    "            \n",
    "            doc = nlp(w)\n",
    "            if doc[0].tag_ == 'NNP':\n",
    "                proper = True\n",
    "            else:\n",
    "                proper = False\n",
    "            \n",
    "            if w in dict:\n",
    "                in_dict = True\n",
    "            else:\n",
    "                in_dict = False\n",
    "                \n",
    "            if w not in stop_words and proper == False and in_dict == True:\n",
    "                filter_sentence.append(w)\n",
    "            \n",
    "        filtered_sentence.append(filter_sentence)\n",
    "    \n",
    "   \n",
    "    synonyms = []\n",
    "    for filtered_num in range(0, len(filtered_sentence)):\n",
    "        for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "            word_synonym = []\n",
    "            for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                for l in syn.lemmas():\n",
    "                    syn_word = l.name()\n",
    "                    try:\n",
    "                        syn_word = syn_word.replace(\"_\", \" \")\n",
    "                    except:\n",
    "                        pass\n",
    "                    word_synonym.append(syn_word)\n",
    "            synonyms.append(word_synonym)\n",
    "    for num in range(0, len(rake_job_keyword[list_num])):\n",
    "        rake_job_keyword[list_num][num] = synonyms[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4887c",
   "metadata": {},
   "source": [
    "It is a little different for thr `yake` model, because the `yake` model returns the keyword with the a float, so we have save the pictures into a different list without the float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_job_keyword = []\n",
    "for f in range(0, len(job_link)):\n",
    "    yake_job = []\n",
    "    for word in raw_yake_job_keyword[f]:\n",
    "        yake_job.append(word[0])\n",
    "    yake_job_keyword.append(yake_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_num in range(0, len(yake_job_keyword)):\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for word_num in range(0, len(yake_job_keyword[list_num])):\n",
    "        keyword_phrase = yake_job_keyword[list_num][word_num]\n",
    "        word_tokens = word_tokenize(keyword_phrase)\n",
    "        \n",
    "        filter_sentence = []\n",
    "        for w in word_tokens:\n",
    "            \n",
    "            doc = nlp(w)\n",
    "            if doc[0].tag_ == 'NNP':\n",
    "                proper = True\n",
    "            else:\n",
    "                proper = False\n",
    "            \n",
    "            if w in dict:\n",
    "                in_dict = True\n",
    "            else:\n",
    "                in_dict = False\n",
    "                \n",
    "            if w not in stop_words and proper == False and in_dict == True:\n",
    "                filter_sentence.append(w)\n",
    "            \n",
    "        filtered_sentence.append(filter_sentence)\n",
    "    \n",
    "   \n",
    "    synonyms = []\n",
    "    for filtered_num in range(0, len(filtered_sentence)):\n",
    "        for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "            word_synonym = []\n",
    "            for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                for l in syn.lemmas():\n",
    "                    syn_word = l.name()\n",
    "                    try:\n",
    "                        syn_word = syn_word.replace(\"_\", \" \")\n",
    "                    except:\n",
    "                        pass\n",
    "                    word_synonym.append(syn_word)\n",
    "            synonyms.append(word_synonym)\n",
    "    for num in range(0, len(yake_job_keyword[list_num])):\n",
    "        yake_job_keyword[list_num][num] = synonyms[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bce50",
   "metadata": {},
   "source": [
    "We now have to calculate the percentage of keywords that are in the resume. We will define a function called `calculate_percentage` that will calculate the percentage given the two parameters `point` and `length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage(point, length):\n",
    "    decimal = num / length\n",
    "    return  decimal*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_percent = []\n",
    "rake_percent = []\n",
    "avg_percent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(0, len(job_link)):\n",
    "    rake_point = 0\n",
    "    for keyword in rake_job_keyword[q]:\n",
    "        for key in keyword:\n",
    "            if key in resume:\n",
    "                rake_point += 1\n",
    "                break\n",
    "    rake_percent.append(calculate_percentage(rake_point, len(rake_job_keyword[q])))\n",
    "    \n",
    "for q in range(0, len(job_link)):\n",
    "    yake_point = 0\n",
    "    for keyword in yake_job_keyword[q]:\n",
    "        for key in keyword:\n",
    "            if key in resume:\n",
    "                yake_point += 1 \n",
    "                break\n",
    "    yake_percent.append(calculate_percentage(yake_point, len(yake_job_keyword[q])))\n",
    "    \n",
    "for g in range(0, len(job_link)):\n",
    "    avg_percent.append(mean([yake_percent[g], rake_percent[g]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef1f9c",
   "metadata": {},
   "source": [
    "Now we can make it into a dataframe, for ease of use, we should sort the values of the dataset based on `avg_percent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75666c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_dict)\n",
    "data = data.sort_values(by = 'avg_percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7937f",
   "metadata": {},
   "source": [
    "###  Part 3: Making it a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f04b0",
   "metadata": {},
   "source": [
    "Now that we have got the code, let us make it easy for the user by making it a function, where he can call it with the nessasary parameters. Let us also add some errors into our code in case it is not inputed the correct parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3765191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_find(url, scroll_num , model_name = 'both'):\n",
    "    import os \n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "    from time import sleep\n",
    "    from time import time\n",
    "    import yake\n",
    "    import spacy\n",
    "    from rake_nltk import Rake\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from statistics import mean\n",
    "    \n",
    "    start_time = time()\n",
    "    os.chdir(r\"C:\\Users\\Aakash\\Desktop\\AAKASH\\Coding Stuff\\Python\\Project\\Linkedin Project\")\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    sleep(3)\n",
    "    action = ActionChains(driver)\n",
    "\n",
    "    for scroll in range(0, scroll_num):\n",
    "        try:\n",
    "            element = driver.find_element_by_link_text(\"See more jobs\")\n",
    "            action.click(element).preform()\n",
    "            #See More Jobs Button to not being clicked \n",
    "        except:\n",
    "            sleep(2) # Has time to load\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "    source = driver.page_source\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    job_link = []\n",
    "    \n",
    "    for a in soup.find_all('a', 'base-card__full-link',href=True):\n",
    "        job_link.append(a['href'])\n",
    "        \n",
    "    class WebLinkError(Exception):\n",
    "        def __init__(self, link, message='is a invalid Linkedin url!'):\n",
    "            self.link = link\n",
    "            self.message = message\n",
    "            super().__init__(self.message)\n",
    "        def __str__(self):\n",
    "            return f'{self.link} {self.message}'\n",
    "        \n",
    "    class ModelError(Exception):\n",
    "        def __init__(self, model ,message = 'is not a valid model. Use rake model or yake model'):\n",
    "            self.model = model\n",
    "            self.message = message\n",
    "            super().__init__(self.message)\n",
    "        def __str__(self):\n",
    "            return f\"{self.model}{self.message}\"\n",
    "    def calculate_percentage(num, length):\n",
    "        decimal = num / length\n",
    "        return  decimal*100\n",
    "        \n",
    "\n",
    "    raw_job_title = []\n",
    "    raw_company_name = []\n",
    "    raw_location = []\n",
    "    raw_job_description = []\n",
    "    raw_level = []\n",
    "    raw_function = []\n",
    "\n",
    "    for x in range(0, len(job_link)):\n",
    "        driver.get(job_link[x])\n",
    "        sleep(2)\n",
    "        \n",
    "        url_count = 0 \n",
    "        url_flag = True\n",
    "        while url_flag == True:\n",
    "            current_url = driver.current_url\n",
    "            if current_url != job_link[x]:\n",
    "                driver.get(job_link[x])\n",
    "                url_count =+ 1\n",
    "            else: \n",
    "                url_flag = False\n",
    "            if url_count == 10:\n",
    "                raise WebLinkError(job_link[x])\n",
    "        \n",
    "        job_source = driver.page_source\n",
    "        soup = BeautifulSoup(job_source, 'lxml')\n",
    "        \n",
    "        raw_job_title.append(soup.find('h1', class_='top-card-layout__title topcard__title'))\n",
    "        raw_company_name.append(soup.find('a', class_ = 'topcard__org-name-link topcard__flavor--black-link'))\n",
    "        raw_location.append(soup.find('span', class_='topcard__flavor topcard__flavor--bullet'))\n",
    "        raw_job_description.append(soup.find('div', class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5'))\n",
    "        raw_level.append(soup.find('span', class_= \"description__job-criteria-text description__job-criteria-text--criteria\")) \n",
    "        raw_function.append(soup.find('span',class_= 'description__job-criteria-text description__job-criteria-text--criteria'))\n",
    "        \n",
    "        sleep(2)\n",
    "        \n",
    "    driver.close()\n",
    "\n",
    "    if len(job_link) == len(raw_job_title) == len(raw_company_name) == len(raw_location) == len(raw_job_description) == len(raw_level) ==len(raw_function):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Lengh of job information list is mismatched.\")\n",
    "    \n",
    "    job_title = []\n",
    "    company_name = []\n",
    "    location = []\n",
    "    job_description = []\n",
    "    level = []\n",
    "    function = []\n",
    "\n",
    "    for y in range(0, len(job_link)):\n",
    "        job_title.append(raw_job_title[y].text)\n",
    "        company_name.append(raw_company_name[y].text)\n",
    "        location.append(raw_location[y].text)\n",
    "        level.append(raw_level[y].text)\n",
    "        function.append(raw_function[y].text)\n",
    "        job_description.append(raw_job_description[y].get_text())\n",
    "\n",
    "    len(job_link)\n",
    "    len(job_title)\n",
    "    len(company_name)\n",
    "    len(location)\n",
    "    len(job_description)\n",
    "    len(level)\n",
    "    len(function)\n",
    "    \n",
    "    if len(job_link) == len(job_title) == len(company_name) == len(location) == len(job_description) == len(level) ==len(function):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Lengh of job information list is mismatched.\")\n",
    "    \n",
    "    for x in range(0, len(job_description)):\n",
    "        job_description[x] = re.sub(\"[^A-Za-z\" \"]+\",\" \",job_description[x]).lower()\n",
    "        job_description[x] = re.sub(\"[0-9\" \"]+\",\" \", job_description[x])\n",
    "    \n",
    "    resume_file = open('resume.txt', 'r', encoding='utf-8')\n",
    "    resume = resume_file.read()\n",
    "    resume_file.close()\n",
    "    \n",
    "    resume = re.sub(\"[^A-Za-z\" \"]+\",\" \",resume).lower()\n",
    "    resume = re.sub(\"[0-9\" \"]+\",\" \", resume)\n",
    "    \n",
    "    dict_file = open('dict.txt', 'r', encoding='utf-8')\n",
    "    dict = dict_file.read()\n",
    "    dict_file.close()\n",
    "    \n",
    "    raw_dict = re.sub(\"[^A-Za-z\" \"]+\",\" \",dict).lower()\n",
    "    raw_dict = re.sub(\"[0-9\" \"]+\",\" \", dict)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dict_tokens = word_tokenize(raw_dict)\n",
    "\n",
    "    dict = []\n",
    "    for w in dict_tokens:\n",
    "        if w not in stop_words:\n",
    "            dict.append(w)\n",
    "    \n",
    "    if model_name == 'both':\n",
    "    \n",
    "        language = 'en'\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.5\n",
    "        numOfKeywords = 30\n",
    "    \n",
    "        custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "        rake_nltk_var = Rake()\n",
    "        \n",
    "        raw_yake_job_keyword = []\n",
    "        rake_job_keyword = []\n",
    "        \n",
    "        for z in range(0, len(job_link)):\n",
    "            raw_yake_job_keyword.append(list(custom_kw_extractor.extract_keywords(job_description[z])))\n",
    "            rake_nltk_var.extract_keywords_from_text(job_description[z])\n",
    "            rake_job_keyword.append(rake_nltk_var.get_ranked_phrases())\n",
    "        \n",
    "        for list_num in range(0, len(rake_job_keyword)):\n",
    "            \n",
    "            filtered_sentence = []\n",
    "            for word_num in range(0, len(rake_job_keyword[list_num])):\n",
    "                keyword_phrase = rake_job_keyword[list_num][word_num]\n",
    "                word_tokens = word_tokenize(keyword_phrase)\n",
    "                \n",
    "                filter_sentence = []\n",
    "                for w in word_tokens:\n",
    "                    \n",
    "                    doc = nlp(w)\n",
    "                    if doc[0].tag_ == 'NNP':\n",
    "                        proper = True\n",
    "                    else:\n",
    "                        proper = False\n",
    "                    \n",
    "                    if w in dict:\n",
    "                        in_dict = True\n",
    "                    else:\n",
    "                        in_dict = False\n",
    "                        \n",
    "                    if w not in stop_words and proper == False and in_dict == True:\n",
    "                        filter_sentence.append(w)\n",
    "                    \n",
    "                filtered_sentence.append(filter_sentence)\n",
    "            \n",
    "           \n",
    "            synonyms = []\n",
    "            for filtered_num in range(0, len(filtered_sentence)):\n",
    "                for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "                    word_synonym = []\n",
    "                    for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                        for l in syn.lemmas():\n",
    "                            syn_word = l.name()\n",
    "                            try:\n",
    "                                syn_word = syn_word.replace(\"_\", \" \")\n",
    "                            except:\n",
    "                                pass\n",
    "                            word_synonym.append(syn_word)\n",
    "                    synonyms.append(word_synonym)\n",
    "            for num in range(0, len(rake_job_keyword[list_num])):\n",
    "                rake_job_keyword[list_num][num] = synonyms[num]\n",
    "        \n",
    "        \n",
    "        yake_job_keyword = []\n",
    "        for f in range(0, len(job_link)):\n",
    "            yake_job = []\n",
    "            for word in raw_yake_job_keyword[f]:\n",
    "                yake_job.append(word[0])\n",
    "            yake_job_keyword.append(yake_job)\n",
    "            \n",
    "        for list_num in range(0, len(yake_job_keyword)):\n",
    "            \n",
    "            filtered_sentence = []\n",
    "            for word_num in range(0, len(yake_job_keyword[list_num])):\n",
    "                keyword_phrase = yake_job_keyword[list_num][word_num]\n",
    "                word_tokens = word_tokenize(keyword_phrase)\n",
    "                \n",
    "                filter_sentence = []\n",
    "                for w in word_tokens:\n",
    "                    \n",
    "                    doc = nlp(w)\n",
    "                    if doc[0].tag_ == 'NNP':\n",
    "                        proper = True\n",
    "                    else:\n",
    "                        proper = False\n",
    "                    \n",
    "                    if w in dict:\n",
    "                        in_dict = True\n",
    "                    else:\n",
    "                        in_dict = False\n",
    "                        \n",
    "                    if w not in stop_words and proper == False and in_dict == True:\n",
    "                        filter_sentence.append(w)\n",
    "                    \n",
    "                filtered_sentence.append(filter_sentence)\n",
    "            \n",
    "           \n",
    "            synonyms = []\n",
    "            for filtered_num in range(0, len(filtered_sentence)):\n",
    "                for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "                    word_synonym = []\n",
    "                    for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                        for l in syn.lemmas():\n",
    "                            syn_word = l.name()\n",
    "                            try:\n",
    "                                syn_word = syn_word.replace(\"_\", \" \")\n",
    "                            except:\n",
    "                                pass\n",
    "                            word_synonym.append(syn_word)\n",
    "                    synonyms.append(word_synonym)\n",
    "            for num in range(0, len(yake_job_keyword[list_num])):\n",
    "                yake_job_keyword[list_num][num] = synonyms[num]\n",
    "\n",
    "        yake_percent = []\n",
    "        rake_percent = []\n",
    "        avg_percent = []\n",
    "        \n",
    "        for q in range(0, len(job_link)):\n",
    "            rake_point = 0\n",
    "            for keyword in rake_job_keyword[q]:\n",
    "                for key in keyword:\n",
    "                    if key in resume:\n",
    "                        rake_point += 1\n",
    "                        break\n",
    "            rake_percent.append(calculate_percentage(rake_point, len(rake_job_keyword[q])))\n",
    "            \n",
    "        for q in range(0, len(job_link)):\n",
    "            yake_point = 0\n",
    "            for keyword in yake_job_keyword[q]:\n",
    "                for key in keyword:\n",
    "                    if key in resume:\n",
    "                        yake_point += 1 \n",
    "                        break\n",
    "            yake_percent.append(calculate_percentage(yake_point, len(yake_job_keyword[q])))\n",
    "            \n",
    "        for g in range(0, len(job_link)):\n",
    "            avg_percent.append(mean([yake_percent[g], rake_percent[g]]))\n",
    "            \n",
    "        data_dict = {\n",
    "            'link':job_link,\n",
    "            'job_title' : job_title,\n",
    "            'company_name':company_name,\n",
    "            'location': location,\n",
    "            'job_description' : job_description,\n",
    "            'yake_keywords' : yake_job_keyword,\n",
    "            'rake_keywords' : rake_job_keyword,\n",
    "            'level':level,\n",
    "            'function':function,\n",
    "            'yake_percent': yake_percent,\n",
    "            'rake_percent': rake_percent,\n",
    "            'avg_percent': avg_percent}\n",
    "        \n",
    "        data = pd.DataFrame(data_dict)\n",
    "        data = data.sort_values(by = 'avg_percent')\n",
    "        return data\n",
    "        \n",
    "    elif model_name == 'rake':\n",
    "        rake_nltk_var = Rake()\n",
    "        \n",
    "        rake_job_keyword = []\n",
    "        \n",
    "        for z in range(0, len(job_link)):\n",
    "            rake_nltk_var.extract_keywords_from_text(job_description[z])\n",
    "            rake_job_keyword.append(rake_nltk_var.get_ranked_phrases())\n",
    "        \n",
    "        for list_num in range(0, len(rake_job_keyword)):\n",
    "            \n",
    "            filtered_sentence = []\n",
    "            for word_num in range(0, len(rake_job_keyword[list_num])):\n",
    "                keyword_phrase = rake_job_keyword[list_num][word_num]\n",
    "                word_tokens = word_tokenize(keyword_phrase)\n",
    "                \n",
    "                filter_sentence = []\n",
    "                for w in word_tokens:\n",
    "                    \n",
    "                    doc = nlp(w)\n",
    "                    if doc[0].tag_ == 'NNP':\n",
    "                        proper = True\n",
    "                    else:\n",
    "                        proper = False\n",
    "                    \n",
    "                    if w in dict:\n",
    "                        in_dict = True\n",
    "                    else:\n",
    "                        in_dict = False\n",
    "                        \n",
    "                    if w not in stop_words and proper == False and in_dict == True:\n",
    "                        filter_sentence.append(w)\n",
    "                    \n",
    "                filtered_sentence.append(filter_sentence)\n",
    "                   \n",
    "            synonyms = []\n",
    "            for filtered_num in range(0, len(filtered_sentence)):\n",
    "                for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "                    word_synonym = []\n",
    "                    for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                        for l in syn.lemmas():\n",
    "                            syn_word = l.name()\n",
    "                            try:\n",
    "                                syn_word = syn_word.replace(\"_\", \" \")\n",
    "                            except:\n",
    "                                pass\n",
    "                            word_synonym.append(syn_word)\n",
    "                    synonyms.append(word_synonym)\n",
    "            for num in range(0, len(rake_job_keyword[list_num])):\n",
    "                rake_job_keyword[list_num][num] = synonyms[num]\n",
    "\n",
    "            rake_percent = []\n",
    "            avg_percent = []\n",
    "        \n",
    "        for q in range(0, len(job_link)):\n",
    "            rake_point = 0\n",
    "            for keyword in rake_job_keyword[q]:\n",
    "                for key in keyword:\n",
    "                    if key in resume:\n",
    "                        rake_point += 1\n",
    "                        break\n",
    "            rake_percent.append(calculate_percentage(rake_point, len(rake_job_keyword[q])))\n",
    "\n",
    "        data_dict = {\n",
    "            'link':job_link,\n",
    "            'job_title' : job_title,\n",
    "            'company_name':company_name,\n",
    "            'location': location,\n",
    "            'job_description' : job_description,\n",
    "            'rake_keywords' : rake_job_keyword,\n",
    "            'level':level,\n",
    "            'function':function,\n",
    "            'rake_percent': rake_percent}\n",
    "        \n",
    "        data = pd.DataFrame(data_dict)\n",
    "        data = data.sort_values(by = 'rake_percent')\n",
    "        return data\n",
    "                \n",
    "    elif model_name == 'yake':\n",
    "        language = 'en'\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.5\n",
    "        numOfKeywords = 30\n",
    "    \n",
    "        custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "        rake_nltk_var = Rake()\n",
    "        \n",
    "        raw_yake_job_keyword = []\n",
    "        \n",
    "        for z in range(0, len(job_link)):\n",
    "            raw_yake_job_keyword.append(list(custom_kw_extractor.extract_keywords(job_description[z])))\n",
    "         \n",
    "        yake_job_keyword = []\n",
    "        for f in range(0, len(job_link)):\n",
    "            yake_job = []\n",
    "            for word in raw_yake_job_keyword[f]:\n",
    "                yake_job.append(word[0])\n",
    "            yake_job_keyword.append(yake_job)\n",
    "            \n",
    "        for list_num in range(0, len(yake_job_keyword)):\n",
    "            \n",
    "            filtered_sentence = []\n",
    "            for word_num in range(0, len(yake_job_keyword[list_num])):\n",
    "                keyword_phrase = yake_job_keyword[list_num][word_num]\n",
    "                word_tokens = word_tokenize(keyword_phrase)\n",
    "                \n",
    "                filter_sentence = []\n",
    "                for w in word_tokens:\n",
    "                    \n",
    "                    doc = nlp(w)\n",
    "                    if doc[0].tag_ == 'NNP':\n",
    "                        proper = True\n",
    "                    else:\n",
    "                        proper = False\n",
    "                    \n",
    "                    if w in dict:\n",
    "                        in_dict = True\n",
    "                    else:\n",
    "                        in_dict = False\n",
    "                        \n",
    "                    if w not in stop_words and proper == False and in_dict == True:\n",
    "                        filter_sentence.append(w)\n",
    "                    \n",
    "                filtered_sentence.append(filter_sentence)\n",
    "            \n",
    "           \n",
    "            synonyms = []\n",
    "            for filtered_num in range(0, len(filtered_sentence)):\n",
    "                for filtered_word in range(0, len(filtered_sentence[filtered_num])):\n",
    "                    word_synonym = []\n",
    "                    for syn in wordnet.synsets(filtered_sentence[filtered_num][filtered_word]):\n",
    "                        for l in syn.lemmas():\n",
    "                            syn_word = l.name()\n",
    "                            try:\n",
    "                                syn_word = syn_word.replace(\"_\", \" \")\n",
    "                            except:\n",
    "                                pass\n",
    "                            word_synonym.append(syn_word)\n",
    "                    synonyms.append(word_synonym)\n",
    "            for num in range(0, len(yake_job_keyword[list_num])):\n",
    "                yake_job_keyword[list_num][num] = synonyms[num]\n",
    "        \n",
    "        yake_percent = []\n",
    "        avg_percent = []\n",
    "            \n",
    "        for q in range(0, len(job_link)):\n",
    "            yake_point = 0\n",
    "            for keyword in yake_job_keyword[q]:\n",
    "                for key in keyword:\n",
    "                    if key in resume:\n",
    "                        yake_point += 1 \n",
    "                        break\n",
    "            yake_percent.append(calculate_percentage(yake_point, len(yake_job_keyword[q])))\n",
    "            \n",
    "        data_dict = {\n",
    "            'link':job_link,\n",
    "            'job_title' : job_title,\n",
    "            'company_name':company_name,\n",
    "            'location': location,\n",
    "            'job_description' : job_description,\n",
    "            'yake_keywords' : yake_job_keyword,\n",
    "            'level':level,\n",
    "            'function':function,\n",
    "            'yake_percent': yake_percent}\n",
    "        \n",
    "        data = pd.DataFrame(data_dict)\n",
    "        data = data.sort_values(by= 'yake_percent')\n",
    "        return data   \n",
    "    else:\n",
    "        raise ModelError(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1462468",
   "metadata": {},
   "source": [
    "The full code in my github repository, and that is all for this project. Thank you for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
